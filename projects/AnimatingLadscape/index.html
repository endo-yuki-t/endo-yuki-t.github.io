<HTML>
<HEAD>
<META http-equiv="content-type" content="text/html; charset=iso-8859-1">
<TITLE>Animating Landscape: Self-Supervised Learning of Decoupled Motion and Appearance for Single-Image Video Synthesis</TITLE>
<style type="text/css">
#wrap{
  width:900px;
  margin-right:auto;
  margin-left:auto;
}
#wrap2{
  width:1440px;
  margin-right:auto;
  margin-left:auto;
}
</style>
</HEAD>
<BODY bgcolor="#FFFFFF">
<p id="wrap" align="right">[Japanese (under construction)]</p>
<div id="wrap"><H2 align="center">Animating Landscape:<BR>
Self-Supervised Learning of Decoupled Motion and Appearance for Single-Image Video Synthesis</H2>
</div>
<CENTER>

<P><a href="http://www.cgg.cs.tsukuba.ac.jp/~endo/index_en.html">Yuki Endo</a><sup>&dagger;&Dagger;</sup>, <a href="http://kanamori.cs.tsukuba.ac.jp/index.html">Yoshihiro Kanamori</a><sup>&dagger;</sup>, <a href="https://galaxy.val.cs.tut.ac.jp/kuriyama.en.html">Shigeru Kuriyama</a><sup>&Dagger;</sup></P>
<P><sup>&dagger;</sup>University of Tsukuba, <sup>&Dagger;</sup>Toyohashi University of Technology</P>
<P>SIGGRAPH Asia 2019</P>
<BR>
<img src="teaser.jpg" width="600" border="0"><img src="output1.gif" width="300" "border="0">
</CENTER>

<div id="wrap">
<HR boader="1">
<h3>Abstract:</h3>
<p align="justify">Automatic generation of a high-quality video from a single image remains a challenging task despite the recent advances in deep generative models. This paper proposes a method that can create a high-resolution, long-term animation using convolutional neural networks (CNNs) from a single landscape image where we mainly focus on skies and waters. Our key observation is that the <i>motion</i> (e.g., moving clouds) and <i>appearance</i> (e.g., time-varying colors in the sky) in natural scenes have different time scales. We thus learn them separately and predict them with decoupled control while handling future uncertainty in both predictions by introducing latent codes. Unlike previous methods that infer output frames directly, our CNNs predict spatially-smooth intermediate data, i.e., for motion, flow fields for warping, and for appearance, color transfer maps, via self-supervised learning, i.e., without explicitly-provided ground truth. These intermediate data are applied not to each previous output frame, but to the input image only once for each output frame. This design is crucial to alleviate error accumulation in long-term predictions, which is the essential problem in previous recurrent approaches. The output frames can be looped like cinemagraph, and also be controlled directly by specifying latent codes or indirectly via visual annotations. We demonstrate the effectiveness of our method through comparisons with the state-of-the-arts on video prediction as well as appearance manipulation.</p>
<p><b>Keywords</b>: Single-image video synthesis; Time-lapse video; Convolutional neural networks; Optical flow prediction; Appearance manipulation</p>

<HR boader="1">
<h3>Video:</h3>
<CENTER>
<iframe width="640" height="360" src="https://www.youtube.com/embed/HBFMOCFZRTA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</CENTER>
<HR boader="1">
<a name="data"></a>
<h3>Data:</h3>
<ul>
  <li>Training data: Our dataset consists of <a href="https://github.com/weixiong-ur/mdgan">Xiong et al.'s dataset</a>, <a href="http://people.csail.mit.edu/yichangshih/time_lapse/">Shih et al.'s dataset</a>, etc. Please contact Yuki Endo (endo(AT)cs.tsukuba.ac.jp) if you are interested in our dataset. </li>
  <li><a href="http://www.cgg.cs.tsukuba.ac.jp/~endo/projects/AnimatingLandscape/models.zip">Pre-trained models</a> (261MB)</li>
</ul>

<HR boader="1">
<h3>Code:</h3>
<ul>
  <li><a href="https://github.com/endo-yuki-t/Animating-Landscape">PyTorch code</a></li>
</ul>

<HR boader="1">
<h3>Publication:</h3>
<ol>
  <li>Yuki Endo, Yoshihiro Kanamori, Shigeru Kuriyama: Animating Landscape: Self-Supervised Learning of Decoupled Motion and Appearance for Single-Image Video Synthesis," ACM Transactions on Graphics (Proc. of SIGGRAPH Asia 2019), Vol. 38, No. 6, Article No. 175, 2019. [<a href="http://www.cgg.cs.tsukuba.ac.jp/~endo/projects/AnimatingLandscape/animating_landscape_siga19.pdf">PDF</a> (highres, 176MB)] [<a href="http://arxiv.org/abs/1910.07192">PDF</a>(lowres, 9MB)] [<a href="http://www.cgg.cs.tsukuba.ac.jp/~endo/projects/AnimatingLandscape/animating_landscape_siga19.mp4">Movie</a> (334 MB)]
</ol>
<hr boader="1">
<h3>BibTeX Citation</h3>
<div style="padding: 10px; margin-bottom: 10px; border: 1px solid #333333;">
@Article{endoSA2019,<br>
  Title                    = {Animating landscape: self-supervised learning of decoupled motion and appearance for single-image video synthesis},<br>
  Author                   = {Yuki Endo and Yoshihiro Kanamori and Shigeru Kuriyama},<br>
  Journal                  = {ACM Transactions on Graphics (Proc. of SIGGRAPH ASIA 2019)},<br>
  volume    = {38},<br>
  number    = {6},<br>
  pages     = {175:1--175:19},<br>
  year      = {2019}<br>
}</div>
<hr boader="1">
<p>Last modified: Nov. 2020</p>
[<a href="javascript:history.back()">back</a>]
</div>
</BODY>
</HTML>
